# Connect and test connection
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import datetime
from datetime import datetime, timezone, timedelta
import json
import random


# Define the path to your text file
LOG_FILE = "./pii.log"

# The Elastic User 
ELASTIC_USER = "elastic"

# Password for the 'elastic' user generated by Elasticsearch
ELASTIC_PASSWORD = "sadfsadfasdf"

# Found in the 'Manage Deployment' page
ELASTIC_CLOUD_ID = "mydeployment:sdfasdfasdf3VuZC5pbzo0NDMkYjA0NmQ0YjFiYzg5NDM3ZDgxM2YxM2RhZjQ3OGE3MzIkZGJmNTE0OGEwODEzNGEwN2E3M2YwYjcyZjljYTliZWQ="

# If not Elastic Cloud and you have a HOST URL 
ELASTIC_HOST = "<elasticsearch-endpoint>"

DATA_STREAM_TYPE = "logs"
DATA_STREAM_DATASET = "pii"
DATA_STREAM_NAMESPACE= "default"
SERVICE_NAME= "pii-generator"

# Create the client instance uusing elastic cloud 
es = Elasticsearch(
    cloud_id=ELASTIC_CLOUD_ID,
    basic_auth=(ELASTIC_USER, ELASTIC_PASSWORD)
)

# Create the client instance using elastic endpoint 
# es = Elasticsearch(
#     ELASTIC_HOST,
#     basic_auth=(ELASTIC_USER, ELASTIC_PASSWORD)
# )

es = Elasticsearch(
    cloud_id=ELASTIC_CLOUD_ID,
    basic_auth=("elastic", ELASTIC_PASSWORD)
)

print(f"Connection Established : \n{json.dumps(es.info().body,indent=4)}")

# Define the index name (and optionally document type)
data_stream = DATA_STREAM_TYPE+"-"+DATA_STREAM_DATASET+"-"+DATA_STREAM_NAMESPACE

# Get time and create run id
now = datetime.now(timezone.utc).astimezone()
run_id = "load-run-" + str(int(now.timestamp()))

# Read the file and load the data
bulk_size = 500
count = 0
total_count = 0
data = []

# Loop through file
with open(LOG_FILE, "r") as f:
  for line in f:
    
    # Assuming each line represents a single document
    # subtract random time to spread out the logs
    d = timedelta(seconds=(random.randint(0,300)))
    timestamp = (now-d).isoformat()
    document = {
        "@timestamp" : timestamp,
        "message" : line.strip(),
        "service" : {"name": SERVICE_NAME},
        "data_stream": {"dataset" : DATA_STREAM_DATASET, "namespace" : DATA_STREAM_NAMESPACE},
        "run.id" : run_id,
        "file.name" : LOG_FILE,
        "event" : {
         "dataset" : "piitest"}
        }
    action = {
      "_index": data_stream,
      "_op_type": "create",
      "_source": document
      }
    data.append(action)
    count = count + 1
    # print(f"Action: '{action}'")

    # Check to see ready to do insert
    if count >= bulk_size:

      # Do the bulk insert
      try:
        res = bulk(es, data)
        print("Succesful Bulk Response: ", res)
      except Exception as e:
        print(e)

      # Clean up and start a new bulk
      total_count = total_count + count
      data = []
      count = 0

total_count = total_count + count

# Send Last Bulk if needed
# print(f"Sending Last Bulk with '{count}' records ")
try:
  res = bulk(es, data)
  print("Bulk Response: ", res)
except Exception as e:
  print(e)
  
print(f"Successfully indexed {total_count} records to '{data_stream}' for run: {run_id}")